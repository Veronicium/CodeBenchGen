"""Helpful utilities for code generation experiments on Exec-CSN"""

import re
import warnings
from typing import Optional

from transformers import PreTrainedTokenizer

ALPACA_TEMPLATE = """{{ (messages|selectattr('role', 'equalto', 'system')|list|last).content|trim if (messages|selectattr('role', 'equalto', 'system')|list) else 'Below is an instruction that describes a task. Write a response that appropriately completes the request.' }}

{% for message in messages %}
{% if message['role'] == 'user' %}
### Instruction:
{{ message['content']|trim -}}
{% if not loop.last %}


{% endif %}
{% elif message['role'] == 'assistant' %}
### Response:
{{ message['content']|trim -}}
{% if not loop.last %}


{% endif %}
{% elif message['role'] == 'user_context' %}
### Input:
{{ message['content']|trim -}}
{% if not loop.last %}


{% endif %}
{% endif %}
{% endfor %}
{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}


### Response:
{% endif %}"""


def set_chat_template(model_name: str, tokenizer: PreTrainedTokenizer) -> None:
    """
    Sets chat templates for HF tokenizers that don't have them (but should have them).

    If you plan to run additional models, you may need to set chat templates here
    (if they are not already set).
    """
    if getattr(tokenizer, "chat_template", None) is not None:
        return

    model_name = model_name.lower()
    if "wizard" in model_name or "speechless" in model_name:
        tokenizer.chat_template = ALPACA_TEMPLATE

    warnings.warn(
        "No chat template found, proceeding with default [INST] [/INST] template."
    )


# Our default instruction template for Exec-CSN problems
INSTRUCTION_TEMPLATE = """\
Complete the {func_name} function in the code below based on the docstring.
Output one complete piece of code. Your code should start with a ```python delimiter and end with a ``` delimiter.

```python
{code}
```"""


# A modified instruction template for CodeLlama models
LLAMA_INSTRUCTION_TEMPLATE = """\
Complete the {func_name} function in the code below based on the docstring.
Output one complete piece of code. Your code should start with a [PYTHON] tag and end with a [/PYTHON] tag.

[PYTHON]
{code}
[/PYTHON]"""


# custom system prompt for CodeLLama models, based on the CodeLlama technical report
LLAMA_SYSTEM_PROMPT = "You are a helpful and intelligent Python programming assistant."

# names of the Llama models used in our experiments
LLAMA_MODELS = {
    "codellama/CodeLlama-7b-Instruct-hf",
    "codellama/CodeLlama-34b-Instruct-hf",
    "codellama/CodeLlama-70b-Instruct-hf",
}


def get_instruction_template(model_name: str) -> str:
    """
    Returns the appropriate instruction template based on the model family
    """
    if model_name in LLAMA_MODELS:
        return LLAMA_INSTRUCTION_TEMPLATE
    else:
        return INSTRUCTION_TEMPLATE


def get_system_prompt(model_name: str) -> Optional[str]:
    """
    Returns the appropriate system prompt for the model family
    (or None if it is not necessary).
    """
    model_name = model_name.lower()
    if model_name in LLAMA_MODELS or "speechless" in model_name:
        return LLAMA_SYSTEM_PROMPT


CODE_BLOCK_PATTERN = r"```(\w*)\n(.*?)\n```"
LLAMA_CODE_BLOCK_PATTERN = r"(```|\[PYTHON\])(([ ]|\w)*)\n((.|\n)*?)\n(```|\[/PYTHON\])"


def extract_code(completion: str, model_name: str) -> str:
    """
    Extracts code snippet from a text completion generated by a decoder
    """
    if model_name in LLAMA_MODELS:
        pattern = LLAMA_CODE_BLOCK_PATTERN
        match = re.findall(pattern, completion)
        if match:
            return max((m[3] for m in match), key=lambda x: len(x))
    else:
        pattern = CODE_BLOCK_PATTERN
        match = re.findall(pattern, completion, flags=re.DOTALL)
        if match:
            # assume longest code block is desired code
            return max((m[1] for m in match), key=lambda x: len(x))
    return ""


def get_stop(model_name: str, disable=False) -> list[str]:
    """
    Returns appropriate stop sequence for model family.
    Using a stop sequence can sometimes prematurely cut off the generated code,
    resulting in a parsing error. However, it does not cause problems in the
    large majority of cases and significantly speeds up inference.
    """
    if disable:
        return []
    if model_name in LLAMA_MODELS:
        return ["<step>", "[/PYTHON]"]
    else:
        # this runs the risk of sometimes terminating at the start of the code
        # block, but this saves so much time that it's probably worth doing.
        return ["```\n\n"]


def get_docstring_indent(code: str, docstring_start: int) -> str:
    """Gets spaces preceding function docstring"""
    # find location of previous newline
    for i in range(docstring_start - 1, -1, -1):
        if code[i] == "\n":
            break
    n_whitespace = docstring_start - i - 1
    return n_whitespace * " "


def replace_docstring(code: str, old_docstring: str, new_docstring: str) -> str:
    """
    Replaces old_docstring in code with new_docstring.
    Used for replacing the original docstrings of the target methods
    with the revised instructions.
    """
    pattern = make_docstring_pattern(old_docstring)
    match = re.search(pattern, code, re.DOTALL)
    if match is None:
        return ""

    indent = get_docstring_indent(code, match.start())
    new_docstring = new_docstring.replace("\n", "\n" + indent)
    return re.sub(pattern, new_docstring, code, count=1)


REGEX_SPECIAL = "\\, ., +, *, ?, ^, $, (, ), [, ], {, }, |".split(", ")


def make_docstring_pattern(docstring: str) -> str:
    """Converts docstring into a regular expression to enable substitution"""
    for sp in REGEX_SPECIAL:
        docstring = docstring.replace(sp, f"\\{sp}")
    return (r"\s*\n\s*").join([l.strip() for l in docstring.splitlines()])
